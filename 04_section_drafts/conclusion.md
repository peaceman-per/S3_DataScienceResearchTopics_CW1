## Conclusion and Discussion

This review has examined how large language models balance factual accuracy and creative generation, revealing a field transitioning from definitional debates toward mechanistic understanding and practical intervention. Significant progress has been achieved in taxonomic refinement, distinguishing intrinsic from extrinsic hallucinations, faithfulness from factuality, and creativity from fabrication. Competing causal accounts—overconfidence from hard-label supervision, knowledge overshadowing, and lifecycle-wide systemic factors—provide complementary intervention points. Methodologically, evaluation has matured beyond surface overlap metrics toward entailment classifiers, dynamic benchmarking, and multi-metric frameworks. Mitigation strategies now span the full model lifecycle: retrieval-augmented generation, reasoning-based methods, training interventions, and decoding techniques. The emerging consensus that hybrid approaches show greatest promise reflects pragmatic accommodation to hallucination's multi-causal nature.

Critically, recognition of second-order trade-offs marks a conceptual advance. The field now acknowledges that mitigation introduces precision-recall tensions, accuracy-informativeness trade-offs, and truthfulness-safety entanglements, fundamentally shifting focus from eliminating hallucination to *managing* it within multi-objective alignment frameworks.

Despite progress, substantial limitations persist. Philosophically, the lack of normative frameworks for evaluating creative generation remains problematic; the creativity-hallucination boundary is undertheorised, with evaluation protocols remaining pragmatic rather than principled. Methodologically, benchmark coverage favours QA and summarisation over dialogue, code generation, and multilingual contexts. LLM-as-judge reliability requires systematic validation, and human evaluation absence in some studies limits real-world applicability claims. Causally, proposed mechanisms require broader validation—knowledge distillation's potential at pretraining scale, scaling laws' ecological validity for natural corpora, and representational entanglement across diverse architectures remain underspecified. These limitations arise from the field's relative youth and hallucination's complexity as an umbrella term for distinct failure modes with different aetiologies.

Addressing these gaps requires coordinated advances across multiple dimensions. Philosophically, engagement with epistemology and normative frameworks would ground evaluation principles. Methodologically, expanding benchmark coverage, validating LLM judges systematically, and integrating human evaluation would strengthen validity. Causally, systematic mechanistic investigations across diverse architectures and scales would deepen causal understanding. Pragmatically, comprehensive computational overhead analyses and abstention policies would inform practical deployment decisions.

This review identifies promising MSc research directions: developing theoretically grounded evaluation frameworks balancing novelty with accuracy, comparing mitigation strategies' computational trade-offs, investigating representational entanglement using interpretability techniques, extending research to underserved domains, and operationalising multi-objective evaluation frameworks. The fundamental tension between factual accuracy and creative generation will remain central to generative AI research. Progress requires philosophical clarity about system expectations and methodological rigour in measurement. As LLMs become increasingly capable and widely deployed, the stakes of achieving this balance continue to grow.
