## Background

Large language models (LLMs) have achieved remarkable advances in natural language generation, demonstrating capabilities that range from factual question answering to creative writing. However, these systems exhibit a persistent tendency to produce outputs that are fluent and coherent yet factually inaccurate or unsupported by evidence—a phenomenon widely termed "hallucination" (Alansari & Luqman, 2025). This propensity poses fundamental challenges to the deployment of generative AI in domains requiring high epistemic reliability, such as medical diagnosis, legal reasoning, and knowledge-grounded summarisation. The tension between factual accuracy and generative flexibility has emerged as a central concern in contemporary AI research, with implications for both system design and evaluation methodology.

The origins of hallucination in LLMs can be traced to the probabilistic foundations of language modelling itself. Trained to predict token sequences from vast corpora, these models encode statistical regularities without inherent mechanisms for truth verification or source attribution. Early research distinguished between *intrinsic hallucinations*—outputs inconsistent with provided context—and *extrinsic hallucinations*—claims unsupported by the model's training data (Bang et al., 2025). This taxonomy reflects a broader epistemological distinction: hallucination concerns consistency with accessible sources, whereas factuality pertains to external truth (Bang et al., 2025). The boundary between intentional creativity and unintentional fabrication remains contested, with Alansari & Luqman (2025) acknowledging a "gray zone" where novelty and accuracy trade off in ways that defy simple categorisation.

Understanding the causal mechanisms underlying hallucination has proven essential for mitigation. Alansari & Luqman (2025) propose a lifecycle framework spanning data collection, architectural design, pretraining, fine-tuning, and inference, arguing that hallucinations emerge from systemic factors rather than isolated decoding failures. Recent work has identified specific mechanisms: Nguyen et al. (2025) attribute faithfulness hallucinations to overconfidence induced by hard-label supervision during fine-tuning, while Zhang et al. (2025) propose a *knowledge overshadowing* hypothesis, wherein dominant knowledge representations suppress less prominent but equally valid information during generation. Notably, Zhang et al. (2025) demonstrate that hallucination rates can increase log-linearly with model size under conditions of knowledge competition, challenging assumptions that scale uniformly improves factual reliability.

The proliferation of hallucination research has been accompanied by methodological advances in detection and evaluation. Traditional overlap metrics such as ROUGE and BLEU prove inadequate for assessing factual consistency, prompting the development of entailment-based classifiers, uncertainty quantification methods, and self-consistency frameworks (Alansari & Luqman, 2025). Bang et al. (2025) advocate for dynamic test set generation to resist benchmark saturation, while Liu et al. (2025) demonstrate iterative prompt-based refinement using verified question-answer pairs. The field increasingly recognises that robust evaluation requires multiple complementary metrics, with growing reliance on LLM-as-judge frameworks despite acknowledged limitations regarding bias propagation and domain transferability.

Mitigation strategies span the full model lifecycle. Retrieval-augmented generation (RAG) and knowledge graph integration provide external grounding at inference time, while reasoning-based methods such as Chain-of-Thought and self-verification leverage internal consistency checks (Alansari & Luqman, 2025). Training-time interventions include knowledge distillation with soft labels to reduce overconfidence (Nguyen et al., 2025) and decoding-time techniques such as contrastive decoding to amplify overshadowed knowledge (Zhang et al., 2025). However, no single approach robustly prevents hallucination across tasks and domains; hybrid methods combining retrieval, reasoning, and calibration appear most promising (Alansari & Luqman, 2025).

Critically, hallucination mitigation introduces second-order trade-offs that complicate optimisation. Liu et al. (2025) observe that excessive optimisation for faithfulness can reduce informational coverage in summarisation tasks, while Mahmoud et al. (2025) identify a truthfulness-safety trade-off whereby interventions that enhance factual accuracy can degrade safety-aligned refusal behaviours. This latter finding suggests that hallucination and refusal may be entangled in shared representational subspaces, necessitating multi-objective evaluation frameworks that balance truthfulness, creativity, and alignment. Such interdependencies underscore the inadequacy of treating hallucination as an isolated failure mode and highlight the need for holistic approaches to model evaluation and refinement.

Looking forward, several challenges remain underexplored. Benchmark coverage remains limited for dialogue systems, code generation, and multilingual contexts (Alansari & Luqman, 2025). The boundary between creativity and hallucination lacks philosophical grounding, with creative generation quality measured indirectly through task performance rather than normative criteria. Causally, the mechanisms by which fine-tuning on benign data can degrade orthogonal capabilities—such as refusal or linguistic register maintenance—are poorly understood (Mahmoud et al., 2025). Computational overhead and practical scalability considerations for hybrid mitigation strategies also warrant systematic investigation.

Against this backdrop, the present review addresses the following research question: *How do large language models balance factual accuracy and creative generation, and what methodological approaches have been proposed to mitigate hallucination in Generative AI systems?* This question encompasses both empirical evaluation frameworks and causal mechanisms, foregrounding the tension between generative flexibility and epistemic reliability that defines contemporary research in this domain. The review synthesises findings from recent benchmark-driven studies, mechanistic investigations, and mitigation proposals, with particular attention to the interdependencies and trade-offs that complicate efforts to align LLM behaviour with human expectations of truthfulness and creativity.
