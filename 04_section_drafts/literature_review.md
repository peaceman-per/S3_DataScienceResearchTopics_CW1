## Review of the Literature

Recent research has disaggregated hallucination into taxonomies, causal mechanisms, and intervention points across the model lifecycle. This review synthesises findings from seven studies, organising the analysis thematically to highlight convergent insights, methodological tensions, and unresolved challenges.

### Defining and Taxonomizing Hallucination

The foundational challenge concerns the boundary between legitimate generative output and epistemic failure. Bang et al. (2025) distinguish *hallucination* (inconsistency with model-accessible sources) from *factuality* (correspondence with external truth), further subdividing hallucination into *intrinsic* (contradicting provided context) and *extrinsic* (unsupported by training data). Nguyen et al. (2025) alternatively foreground *faithfulness hallucination* (ungrounded from context) versus *factuality hallucination* (world-knowledge contradictions), prioritising the attribution problem. Alansari & Luqman (2025) acknowledge a "gray zone" where intentional creativity and unintentional fabrication become indistinguishable. The lack of consensus reflects deeper questions: should LLMs be evaluated as retrieval systems (where unsupported claims fail) or creative agents (where novelty is valued)?

These definitional disputes carry methodological consequences. Bang et al. (2025) critique benchmarks like TruthfulQA for conflating factuality with hallucination, arguing this misleads model development. Liu et al. (2025) operationalise hallucination narrowly as factual inconsistency via FactCC entailment classifiers. The choice of definition constrains admissible evaluation protocols and progress measurement.

### Causal Mechanisms and Mitigation

Understanding *why* hallucinations occur enables targeted interventions. Alansari & Luqman (2025) propose a lifecycle framework spanning data, architecture, pretraining, fine-tuning, and inference, arguing hallucinations are systemic emergent behaviours. Nguyen et al. (2025) attribute faithfulness failures to *overconfidence* from hard-label supervision during fine-tuning, demonstrating that knowledge distillation with soft labels reduces hallucination without degrading reasoning capability. Zhang et al. (2025) propose *knowledge overshadowing*—dominant representations suppressing less prominent knowledge—with hallucination rates increasing log-linearly with popularity imbalance, knowledge length, and model size. This scaling law suggests larger models may paradoxically exhibit increased hallucinations under knowledge competition, challenging assumptions that scale uniformly improves reliability.

Mahmoud et al. (2025) identify shared representational entanglement between hallucination and safety-aligned refusal, demonstrating that overlapping attention heads support both capabilities such that interventions targeting one can degrade the other. This challenges the assumption that hallucination mitigation can proceed independently of other alignment objectives.

Mitigation strategies span the lifecycle. Retrieval-augmented generation (RAG) grounds outputs in external evidence but introduces retrieval quality dependencies (Alansari & Luqman, 2025; Bang et al., 2025). Reasoning-based methods like Chain-of-Thought improve transparency but incur computational costs (Alansari & Luqman, 2025). Training-time interventions modify internal representations: Nguyen et al. (2025) demonstrate knowledge distillation's effectiveness, though teacher access imposes overhead. Decoding-time methods offer training-free alternatives: Zhang et al. (2025) propose CoDA, achieving substantial gains on knowledge-conflict benchmarks. Liu et al. (2025) demonstrate post-hoc iterative refinement via QA pairs, optimal after six iterations but requiring reliable fact-checking oracles. Alansari & Luqman (2025) conclude no single method suffices; hybrid approaches combining techniques show greatest promise.

### Evaluation and Detection

Effective mitigation presupposes reliable detection. Alansari & Luqman (2025) taxonomise approaches: retrieval-based, uncertainty-based, embedding-based, learning-based, and self-consistency-based. Liu et al. (2025) propose answer-first QA generation to reduce evaluation-time hallucinations—a reflexive concern that frameworks themselves may hallucinate. However, reliance on learned classifiers like FactCC introduces error sources and limited generalisability.

Standard overlap metrics (ROUGE, BLEU, BERTScore) miss factuality failures, driving adoption of NLI-based classifiers and LLM-as-judge frameworks (Alansari & Luqman, 2025). Yet Bang et al. (2025) and Alansari & Luqman (2025) highlight that LLM judges propagate biases, exhibit poor calibration, and fail to transfer across domains. The pragmatic necessity of scalable evaluation has led to widespread GPT-4-based scoring—an assumption warranting empirical validation. Nguyen et al. (2025) employ attention-based detection via Lookback Lens-style classifiers, offering interpretability but requiring model-specific training. The diversity of paradigms reflects hallucination's multifaceted nature and the absence of gold-standard metrics.

Bang et al. (2025) introduce HalluLens, combining dynamic test generation (PreciseWikiQA, LongWiki) with static benchmarks to resist saturation and data leakage. Dynamic generation from Wikipedia operationalises extrinsic hallucination, though Wikipedia may not match actual training data. Liu et al. (2025) employ multi-metric evaluation (ROUGE-L, FactCC, attention-based classifiers) across CNN/DailyMail, PubMed, and ArXiv. Alansari & Luqman (2025) highlight uneven coverage: QA and summarisation well-represented, dialogue and code generation underserved, constraining generalisability. Bang et al. (2025) critique factuality benchmarks for conflating factuality with hallucination—a tension between measuring what models *know* versus what they *ground claims in*.

### Trade-offs and Research Gaps

Mitigation introduces second-order trade-offs. Liu et al. (2025) observe excessive faithfulness optimisation reduces informational coverage as models become conservative. Bang et al. (2025) quantify precision-recall tension: answering more increases hallucination risk; refusing more sacrifices helpfulness. No Pareto-optimal solution exists across 13 evaluated LLMs. Mahmoud et al. (2025) identify truthfulness-safety trade-offs: factual accuracy interventions can degrade refusal behaviours, increasing jailbreak susceptibility. Shared attention heads entangle these capabilities, such that alignment objectives interact rather than optimise independently. Nguyen et al. (2025) offer rare counter-evidence: knowledge distillation reduces hallucination without capability penalties, though generalisability to pretraining and teacher hallucination propagation remain unaddressed.

Zhang et al.'s (2025) scaling law—hallucination rates increase log-linearly with model size under knowledge competition—complicates optimistic scaling narratives, suggesting architectural innovations may be required. However, synthetic experimental conditions raise ecological validity questions for real-world corpora.

Substantial gaps persist. Philosophically, the literature exhibits limited engagement with epistemology or normative frameworks for creative generation (Alansari & Luqman, 2025). The creativity-hallucination boundary is acknowledged but undertheorised. Methodologically, benchmark choices are pragmatic rather than theoretically justified (Liu et al., 2025; Bang et al., 2025). LLM-as-judge reliability across domains is insufficiently validated (Alansari & Luqman, 2025). Human evaluation absence in some studies (Nguyen et al., 2025) limits real-world applicability claims. Task coverage favours summarisation and QA over creative generation, dialogue, and multilingual contexts (Alansari & Luqman, 2025). Causally, knowledge distillation's application to fine-tuning may understate pretraining potential (Nguyen et al., 2025), and quantifying "popularity" in natural text remains challenging (Zhang et al., 2025). Mahmoud et al.'s (2025) entanglement hypothesis requires broader architectural validation. Pragmatically, computational overhead comparisons and multi-objective evaluation frameworks balancing truthfulness, safety, and helpfulness are not standard practice.

### Synthesis

The field transitions from descriptive taxonomy to mechanistic understanding and intervention. Definitional consensus remains elusive: competing frameworks privilege consistency (Bang et al., 2025), faithfulness (Nguyen et al., 2025), or factuality (Zhang et al., 2025). Causal accounts diverge between overconfidence (Nguyen et al., 2025), knowledge overshadowing (Zhang et al., 2025), and lifecycle factors (Alansari & Luqman, 2025). Yet convergent insights emerge: hallucination is multi-causal, requiring multi-pronged mitigation with hybrid approaches showing promise (Alansari & Luqman, 2025). Evaluation has matured beyond surface metrics toward entailment classifiers, self-consistency checks, and dynamic benchmarking (Bang et al., 2025; Liu et al., 2025). Recognition of trade-offs—accuracy-informativeness (Liu et al., 2025), truthfulness-safety (Mahmoud et al., 2025), refusal-helpfulness (Bang et al., 2025)—shifts framing from eliminating hallucination to *managing* it within multi-objective alignment.

Critically, the field lacks a unified normative stance on generative systems' roles: retrieval systems (where unsupported claims fail), creative agents (where novelty is valued), or hybrid entities navigating context-dependent expectations. Deployment scenarios likely determine the answer, but absent explicit philosophical engagement, progress toward principled evaluation and design is constrained. As research matures, epistemic and normative foundations alongside empirical investigation will be essential for aligning systems with human expectations of truthfulness, creativity, and trustworthiness.
