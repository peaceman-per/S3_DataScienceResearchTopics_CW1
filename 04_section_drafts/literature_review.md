## Review of the Literature

The literature on hallucination in large language models reveals a rapidly evolving field characterised by conceptual refinement, methodological innovation, and growing recognition of systemic trade-offs. Rather than treating hallucination as a monolithic failure mode, recent research has disaggregated the phenomenon into taxonomies, causal mechanisms, and intervention points across the model lifecycle. This review synthesises findings from seven recent studies, organising the analysis thematically to highlight convergent insights, methodological tensions, and unresolved challenges that define the current state of knowledge.

### Defining and Taxonomizing Hallucination

The foundational challenge in hallucination research concerns the boundary between legitimate generative output and epistemic failure. Bang et al. (2025) propose a principled distinction between *hallucination*—defined as inconsistency with model-accessible sources (training data or input context)—and *factuality*, which concerns correspondence with external truth. This formulation reflects an important epistemological stance: hallucination evaluates internal consistency, whereas factuality requires verification against an oracle. Building on this framework, Bang et al. (2025) distinguish *intrinsic hallucination* (output contradicting provided context) from *extrinsic hallucination* (claims unsupported by training data), a taxonomy echoed in other recent work (Alansari & Luqman, 2025).

However, alternative conceptualisations complicate this picture. Nguyen et al. (2025) foreground *faithfulness hallucination*—ungrounded generation from context—as distinct from *factuality hallucination*, which involves world-knowledge contradictions. This distinction prioritises the attribution problem: whether claims are traceable to accessible evidence. Alansari & Luqman (2025) further acknowledge a "gray zone" where intentional creativity and unintentional fabrication become indistinguishable, particularly in tasks requiring abstraction or synthesis. The lack of consensus on definitional boundaries is not merely terminological; it reflects deeper questions about the normative criteria for generative systems. Should LLMs be evaluated as retrieval systems (where unsupported claims are failures) or as creative agents (where novelty is valued)? The field has yet to articulate a unified philosophical stance on this tension.

These definitional disputes carry methodological consequences. Bang et al. (2025) critique popular benchmarks such as TruthfulQA for conflating factuality with hallucination, arguing that factuality benchmarks can mislead model development when hallucination reduction is the intended goal. By contrast, Liu et al. (2025) operationalise hallucination narrowly as factual inconsistency in summarisation, measured via FactCC—a model trained to detect entailment failures. The choice of definition thus constrains the space of admissible evaluation protocols, with implications for how progress is measured and reported.

### Causal Mechanisms and Lifecycle Analysis

Understanding *why* hallucinations occur has emerged as essential for designing targeted interventions. Alansari & Luqman (2025) propose a lifecycle framework spanning data collection, architectural design, pretraining, fine-tuning, and inference, arguing that hallucinations are not isolated decoding failures but systemic emergent behaviours. This holistic perspective contrasts with earlier work that treated hallucination primarily as a sampling or prompt-engineering problem.

Within this lifecycle view, specific causal mechanisms have been identified. Nguyen et al. (2025) attribute faithfulness hallucinations to *overconfidence* induced by hard-label (one-hot) supervision during instruction fine-tuning. Under this hypothesis, deterministic training targets encourage the model to assign near-certainty to next-token predictions, even when multiple plausible continuations exist. Empirically, Nguyen et al. (2025) demonstrate that replacing hard labels with teacher-provided soft probability distributions—via knowledge distillation—reduces hallucination rates without degrading performance on general reasoning benchmarks. This finding suggests that hallucination mitigation need not sacrifice generative capability, challenging earlier assumptions about an inevitable accuracy-creativity trade-off.

An alternative mechanism emerges from Zhang et al. (2025), who propose *knowledge overshadowing*: dominant knowledge representations suppress less prominent but equally valid knowledge during generation. Crucially, Zhang et al. (2025) demonstrate that hallucination rates increase log-linearly with knowledge popularity imbalance, knowledge length, and model size. This scaling law implies that larger, better-generalising models may paradoxically exhibit *increased* hallucinations under conditions of knowledge competition—a counterintuitive result that complicates the assumption that scale uniformly improves reliability. The overshadowing hypothesis also suggests that hallucinations can persist even with "truthful" data, as internal representational competition may misalign retrieval priorities regardless of corpus quality.

These mechanistic accounts are not mutually exclusive, but they privilege different intervention points. Nguyen et al. (2025) advocate training-time intervention via soft supervision, whereas Zhang et al. (2025) propose decoding-time adjustments through contrastive decoding to amplify overshadowed knowledge. The diversity of proposed mechanisms underscores the likelihood that hallucination is not a unitary phenomenon but rather an umbrella term for distinct failure modes with different aetiologies.

A further complication emerges from Mahmoud et al. (2025), who identify shared representational entanglement between hallucination and safety-aligned refusal behaviours. Through mechanistic interpretability analyses, Mahmoud et al. (2025) demonstrate that overlapping attention heads support both truthfulness and refusal, such that interventions targeting one capability can inadvertently degrade the other. This finding challenges the assumption that hallucination mitigation can be pursued independently of other alignment objectives, suggesting instead that model behaviours are interdependent at the level of internal representations.

### Detection Methodologies

Effective mitigation presupposes reliable detection, and recent work has proliferated diverse detection paradigms. Alansari & Luqman (2025) taxonomise detection approaches into retrieval-based, uncertainty-based, embedding-based, learning-based, and self-consistency-based methods. Retrieval-based detection leverages external knowledge sources to verify claims, operationalising hallucination as deviation from retrievable evidence. Uncertainty-based methods exploit the observation that hallucinated outputs often correspond to high model entropy or low token probability, though this correlation is imperfect for high-confidence hallucinations.

Liu et al. (2025) propose an *answer-first QA generation* framework to reduce evaluation-time hallucinations: by generating questions from source documents (rather than from summaries), the method minimises contamination of the evaluation signal itself. This reflexive concern—that evaluation frameworks may themselves hallucinate—represents a significant methodological advance, foregrounding the need for ground-truth-aligned verification pipelines. However, Liu et al. (2025) rely on FactCC, a learned entailment classifier, which introduces its own error sources and may generalise poorly beyond summarisation tasks.

The limitations of automated detection metrics are widely acknowledged. Alansari & Luqman (2025) note that standard overlap metrics (ROUGE, BLEU, BERTScore) often miss factuality failures, as surface-level similarity does not entail semantic correctness. This inadequacy has driven the adoption of NLI-based classifiers and LLM-as-judge frameworks, though both approaches have known limitations. Bang et al. (2025) and Alansari & Luqman (2025) highlight that LLM judges may propagate biases, exhibit poor calibration, and fail to transfer across domains or languages. Despite these concerns, the pragmatic necessity of scalable evaluation has led to widespread reliance on GPT-4-based scoring, reflecting an implicit trust that larger, more capable models provide more reliable judgements—an assumption that warrants empirical validation.

Nguyen et al. (2025) employ attention-based hallucination detection, leveraging Lookback Lens-style factuality classifiers that analyse which source tokens the model attends to during generation. This interpretability-driven approach offers transparency advantages over black-box metrics, though it requires model-specific training and may not generalise across architectures. The diversity of detection paradigms reflects both the multifaceted nature of hallucination and the absence of a gold-standard metric, with each method optimising for different desiderata (scalability, interpretability, task-specificity, or generalisability).

### Mitigation Strategies

Mitigation strategies span prompt engineering, retrieval augmentation, reasoning-based methods, training interventions, and decoding adjustments. Alansari & Luqman (2025) argue that no single method robustly prevents hallucination across tasks and domains, with hybrid approaches combining multiple techniques showing greatest promise.

Retrieval-augmented generation (RAG) has emerged as a dominant paradigm, grounding outputs in externally retrieved evidence to reduce extrinsic hallucinations (Alansari & Luqman, 2025; Bang et al., 2025). However, RAG introduces dependencies on retrieval quality and may struggle with intrinsic hallucinations when provided context is itself inaccurate or incomplete. Reasoning-based methods—such as Chain-of-Thought prompting and self-consistency sampling—leverage internal deliberation to reduce hallucination by encouraging explicit justification and multi-path verification (Alansari & Luqman, 2025). These methods improve transparency but incur computational costs and may fail when all sampled reasoning paths converge on the same hallucinated output.

Training-time interventions aim to modify the model's internal representations before deployment. Nguyen et al. (2025) demonstrate that knowledge distillation during instruction fine-tuning reduces hallucination without sacrificing general reasoning capability, supporting the hypothesis that overconfidence rather than capability drives faithfulness failures. However, this approach requires access to larger teacher models, introducing computational overhead and raising questions about whether teacher hallucinations may propagate to students.

Decoding-time methods offer training-free alternatives. Zhang et al. (2025) propose Contrastive Decoding to Amplify Overshadowed Knowledge (CoDA), which detects tokens suppressed by dominant knowledge and adjusts their probabilities during sampling. Evaluated on benchmarks designed to elicit knowledge conflicts (MemoTrap, NQ-Swap), CoDA achieves substantial exact-match gains. However, its robustness to complex, multi-hop reasoning tasks and its scalability to production settings remain underexplored.

Post-hoc iterative refinement represents another mitigation avenue. Liu et al. (2025) propose a Question-Answer-Sorting-Evaluation framework that iteratively corrects summaries using verified QA pairs, achieving optimal performance after approximately six iterations with diminishing returns thereafter. This method is transparent and model-agnostic but assumes access to reliable fact-checking oracles and may over-correct, reducing informational richness.

A critical observation is that mitigation strategies vary in their timing (pre-training, fine-tuning, inference), computational cost, and transparency. Alansari & Luqman (2025) note that prompt-based methods are lightweight and interpretable but less robust than model-centric training, whereas fine-tuning offers deeper intervention but requires significant resources. The optimal mitigation strategy thus depends on deployment constraints, task requirements, and acceptable trade-offs between accuracy, efficiency, and flexibility.

### Evaluation Frameworks and Benchmarking

The proliferation of hallucination benchmarks reflects both the urgency of the problem and the difficulty of constructing valid evaluation protocols. Bang et al. (2025) introduce HalluLens, which combines dynamic test set generation (PreciseWikiQA, LongWiki) with static intrinsic hallucination benchmarks (HHEM, ANAH 2.0). The dynamic generation approach aims to resist benchmark saturation and data leakage—concerns that arise when models are trained on datasets overlapping with evaluation sets. By generating questions from Wikipedia and using Wikipedia-only verification pipelines, Bang et al. (2025) operationalise extrinsic hallucination in a way that approximates (but does not perfectly match) consistency with actual training data.

Liu et al. (2025) evaluate hallucination mitigation on CNN/DailyMail, PubMed, and ArXiv summarisation benchmarks, employing ROUGE-L, FactCC, and attention-based factuality classifiers. This multi-metric approach reflects growing recognition that no single metric suffices: ROUGE captures surface overlap, FactCC assesses entailment, and attention-based metrics provide interpretability. However, reliance on learned classifiers introduces potential circularity, as FactCC itself may exhibit biases or domain-specific limitations.

Alansari & Luqman (2025) highlight uneven benchmark coverage: QA and summarisation are well-represented, whereas dialogue, code generation, and multilingual contexts remain underserved. This imbalance constrains generalisability and may bias the field toward task-specific solutions that do not transfer broadly. Furthermore, Alansari & Luqman (2025) note that LLM-as-judge frameworks, while pragmatically necessary for scalability, suffer from calibration issues and may not transfer reliably across domains or languages.

Bang et al. (2025) explicitly critique popular factuality benchmarks (e.g., TruthfulQA) for conflating factuality with hallucination, arguing that such benchmarks measure external knowledge rather than consistency with accessible sources. This critique underscores a deeper tension: should evaluation protocols assess what the model *knows* (factuality) or what it *grounds claims in* (hallucination)? The answer depends on intended use cases—retrieval systems prioritise consistency, whereas creative generation may tolerate some extrinsic divergence.

The methodological sophistication of evaluation has advanced considerably, with increasing reliance on task-agnostic signals, self-consistency checks, and reference-free metrics (Alansari & Luqman, 2025). However, Nguyen et al. (2025) report no human evaluation, relying entirely on automated metrics—a limitation that may underestimate subtle errors or subjective quality dimensions. The tension between scalability and validity remains unresolved, with large-scale automated evaluation enabling rapid iteration at the cost of potential misalignment with human judgements.

### Trade-offs and Interdependencies

A defining feature of recent hallucination research is the recognition that mitigation introduces second-order trade-offs. Liu et al. (2025) observe that excessive optimisation for faithfulness can reduce informational coverage, as models become overly conservative and omit relevant details to avoid potential errors. This precision-recall tension is fundamental: answering more questions increases hallucination risk, whereas refusing more often sacrifices helpfulness.

Bang et al. (2025) quantify this trade-off explicitly, measuring false refusal rates alongside hallucination rates across 13 instruction-tuned LLMs. They find substantial variation in how models balance these competing objectives, with no clear Pareto-optimal solution. This variability suggests that abstention policies—mechanisms for models to express uncertainty and decline answering—are critical but underexplored components of hallucination mitigation.

Mahmoud et al. (2025) identify a previously unrecognised truthfulness-safety trade-off: interventions that enhance factual accuracy can degrade safety-aligned refusal behaviours, increasing susceptibility to jailbreaks. Through mechanistic interpretability analyses, Mahmoud et al. (2025) demonstrate that hallucination-related and refusal-related behaviours are partially entangled in shared attention heads, such that even fine-tuning on benign datasets can inadvertently reduce safety. This finding challenges the assumption that alignment objectives (truthfulness, harmlessness, helpfulness) can be optimised independently, suggesting instead that they interact at the level of internal representations.

Nguyen et al. (2025) report that knowledge distillation reduces hallucination without degrading general reasoning capability, offering a rare example where mitigation does not incur capability penalties. However, the generalisability of this result is uncertain: Nguyen et al. (2025) apply distillation to instruction fine-tuning rather than pretraining, and the method requires access to larger teacher models, introducing computational overhead. Furthermore, the potential for teacher hallucinations to propagate to students remains a concern, though not empirically addressed in their work.

Zhang et al. (2025) propose that hallucination rates increase log-linearly with model size under knowledge competition, implying that scale may exacerbate certain factual failures even as it improves other capabilities. This result complicates optimistic narratives about scaling laws and suggests that architectural or training innovations may be required to decouple model size from hallucination propensity.

### Scaling Effects and Model Size

The relationship between model size and hallucination remains contentious. Zhang et al. (2025) derive a log-linear scaling law from controlled synthetic pretraining experiments, demonstrating that factual hallucination rates increase with model size when knowledge competition is present. This finding suggests that larger models, which generalise and compress knowledge more effectively, may paradoxically amplify overshadowing effects. The law predicts hallucination rates with approximately 8% average relative error across fine-tuning tasks spanning time/location relations, negation, logical reasoning, and conflict resolution.

However, the synthetic nature of Zhang et al.'s (2025) experiments raises questions about ecological validity. Real-world training corpora exhibit complex, non-uniform knowledge distributions that may not align with controlled experimental conditions. The applicability of the scaling law to messy, heterogeneous datasets remains an open empirical question. Furthermore, the law quantifies *relative* hallucination rates under specific knowledge competition scenarios, not absolute hallucination propensity across all generative tasks.

Liu et al. (2025) and Nguyen et al. (2025) evaluate models of varying sizes (Llama-2-7B/13B, Llama-3.1-8B/70B, Qwen-2.5-7B/32B) but do not systematically isolate size as an independent variable. Anecdotally, both studies suggest that mitigation techniques (iterative refinement, knowledge distillation) are effective across model sizes, though effect magnitudes may vary. Alansari & Luqman (2025) note that hallucination behaviour varies significantly across models, likely reflecting differences in architecture, training data, and alignment procedures in addition to sheer parameter count.

The absence of systematic scaling studies—controlling for architecture, data, and training regime while varying only size—limits definitive conclusions. Zhang et al.'s (2025) scaling law represents the most direct engagement with this question but remains preliminary pending validation on diverse, naturalistic datasets and SOTA closed-source models.

### Research Gaps and Limitations

Despite rapid progress, substantial gaps remain. Philosophically, the literature exhibits limited engagement with epistemology, theories of truth, or normative frameworks for creative generation (Alansari & Luqman, 2025). The boundary between creativity and hallucination is acknowledged but undertheorised, with creative generation quality measured indirectly through task performance rather than grounded in normative criteria. This conceptual gap constrains the development of principled evaluation frameworks that balance accuracy and novelty.

Methodologically, benchmark design choices significantly influence what counts as hallucination, yet these choices are often pragmatic rather than theoretically justified (Liu et al., 2025; Bang et al., 2025). Detection methods remain imperfect proxies for human judgement, and LLM-as-judge reliability across domains and languages is insufficiently validated (Alansari & Luqman, 2025). The absence of human evaluation in some studies (Nguyen et al., 2025) limits claims about real-world applicability, as automated metrics may miss subtle errors or subjective quality dimensions.

Task and model coverage is uneven. Most work focuses on summarisation and QA, with limited exploration of open-ended creative generation, dialogue, code generation, and multilingual contexts (Alansari & Luqman, 2025; Liu et al., 2025). Evidence for closed-source SOTA models is often anecdotal (Zhang et al., 2025), and generalisability across architectures, alignment recipes, and scale remains uncertain (Mahmoud et al., 2025).

Causally, several mechanisms remain underspecified. Nguyen et al. (2025) acknowledge that knowledge distillation may inherit teacher hallucinations, and their application to instruction fine-tuning (rather than pretraining) may understate potential at scale. Zhang et al. (2025) note that quantifying "popularity" and "length" in natural text—as distinct from synthetic corpora—poses significant measurement challenges. Mahmoud et al.'s (2025) representational entanglement hypothesis requires validation beyond the two models tested, and whether sparse autoencoders can disentangle features reliably across diverse architectures is unproven.

Pragmatically, computational overhead comparisons across mitigation strategies are not comprehensively analysed (Alansari & Luqman, 2025). Teacher inference for knowledge distillation adds cost (Nguyen et al., 2025), as does multi-sampling for self-consistency and retrieval for RAG. Multi-objective evaluation frameworks balancing truthfulness, safety, and helpfulness are not standard practice (Mahmoud et al., 2025), yet the interdependencies among these objectives suggest such frameworks are essential.

Temporally, most papers reviewed are from 2025, with only limited coverage of the 2019-2023 period. This recency bias constrains understanding of how hallucination research methods have evolved and whether current approaches represent continuities or ruptures with earlier work.

### Synthesis

The literature on LLM hallucination reveals a field in transition from descriptive taxonomy to mechanistic understanding and intervention. Definitional consensus remains elusive, with competing frameworks privileging consistency (Bang et al., 2025), faithfulness (Nguyen et al., 2025), or factuality (Zhang et al., 2025) as primary desiderata. Causal accounts similarly diverge, attributing hallucinations to overconfidence (Nguyen et al., 2025), knowledge overshadowing (Zhang et al., 2025), or lifecycle-wide systemic factors (Alansari & Luqman, 2025).

Despite these tensions, convergent insights emerge. Hallucination is now widely understood as a multi-causal phenomenon requiring multi-pronged mitigation, with hybrid approaches combining retrieval, reasoning, and calibration showing greatest promise (Alansari & Luqman, 2025). Evaluation methodology has matured considerably, moving beyond surface overlap metrics toward entailment-based classifiers, self-consistency checks, and dynamic benchmarking (Bang et al., 2025; Liu et al., 2025). Recognition of second-order trade-offs—between accuracy and informativeness (Liu et al., 2025), truthfulness and safety (Mahmoud et al., 2025), and refusal and helpfulness (Bang et al., 2025)—has shifted the framing from eliminating hallucination to *managing* it within a multi-objective alignment framework.

Critically, the field has yet to articulate a unified normative stance on the role of generative systems. Should LLMs function primarily as retrieval systems (where unsupported claims constitute failures), as creative agents (where novelty is valued), or as hybrid entities navigating context-dependent expectations? The answer likely depends on deployment scenarios, but the absence of explicit philosophical engagement with these questions constrains progress toward principled evaluation and design. As hallucination research matures, greater attention to the epistemic and normative foundations of generative AI—alongside continued empirical and mechanistic investigation—will be essential for aligning these systems with human expectations of truthfulness, creativity, and trustworthiness.
