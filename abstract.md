# Abstract

Large language models (LLMs) demonstrate remarkable natural language generation capabilities but exhibit a persistent tendency to produce factually inaccurate or unsupported outputsâ€”a phenomenon termed "hallucination." This review addresses the question: *How do large language models balance factual accuracy and creative generation, and what methodological approaches have been proposed to mitigate hallucination in Generative AI systems?* 

The review synthesises recent research examining hallucination taxonomies, causal mechanisms, and intervention strategies across the model lifecycle. Definitional frameworks distinguish intrinsic hallucinations (contradicting provided context) from extrinsic hallucinations (unsupported by training data), and separate hallucination (consistency with model-accessible sources) from factuality (correspondence with external truth). Proposed causal mechanisms include overconfidence from hard-label supervision during fine-tuning, knowledge overshadowing wherein dominant representations suppress less prominent information, and systemic factors spanning data collection, architecture, pretraining, fine-tuning, and inference.

Evaluation methodologies have evolved beyond surface overlap metrics toward entailment-based classifiers, uncertainty quantification, self-consistency frameworks, and dynamic benchmark generation. Mitigation strategies span retrieval-augmented generation for external grounding, reasoning-based methods such as Chain-of-Thought, training-time interventions including knowledge distillation with soft labels, and decoding-time techniques like contrastive decoding. The emerging consensus indicates that hybrid approaches combining multiple techniques show greatest promise given hallucination's multi-causal nature.

Critically, the field increasingly recognises second-order trade-offs: excessive faithfulness optimisation reduces informational coverage, truthfulness interventions can degrade safety-aligned refusal behaviours, and precision-recall tensions emerge between helpfulness and hallucination risk. These findings shift the focus from eliminating hallucination to managing it within multi-objective alignment frameworks that balance truthfulness, creativity, and safety.

Substantial research gaps persist. Benchmark coverage favours question answering and summarisation over dialogue, code generation, and multilingual contexts. The creativity-hallucination boundary lacks philosophical grounding, with evaluation protocols remaining pragmatic rather than theoretically principled. Proposed causal mechanisms require broader architectural validation, and computational overhead comparisons for mitigation strategies remain unsystematic. Addressing these limitations requires philosophical engagement with epistemic frameworks, methodological expansion of benchmark diversity and human evaluation, mechanistic investigations across architectures and scales, and development of multi-objective evaluation frameworks. As LLMs become increasingly deployed in high-stakes domains, achieving principled balance between factual accuracy and creative generation remains central to generative AI research.
