# Abstract

Large language models (LLMs) demonstrate remarkable natural language generation capabilities. However, they exhibit a persistent tendency to produce factually inaccurate or unsupported outputs—a phenomenon termed "hallucination." This review addresses: *How do large language models balance factual accuracy and creative generation, and what methodological approaches have been proposed to mitigate hallucination in Generative AI systems?*

The review synthesises recent research examining hallucination taxonomies, causal mechanisms, and intervention strategies. Definitional frameworks distinguish intrinsic hallucinations (contradicting provided context) from extrinsic hallucinations (unsupported by training data), and separate hallucination (consistency with model-accessible sources) from factuality (correspondence with external truth). Proposed causal mechanisms include overconfidence from hard-label supervision. They also include knowledge overshadowing wherein dominant representations suppress less prominent information, and systemic factors spanning data collection, architecture, pretraining, fine-tuning, and inference.

Mitigation strategies span retrieval-augmented generation, reasoning-based methods such as Chain-of-Thought, knowledge distillation, and contrastive decoding. Evaluation methodologies has evolved beyond surface overlap metrics toward entailment-based classifiers, uncertainty quantification, and dynamic benchmark generation. The emerging consensus indicates hybrid approaches show greatest promise given hallucination's multi-causal nature.

Critically, the field recognises second-order trade-offs: excessive faithfulness optimisation reduces informational coverage. Truthfulness interventions can degrade safety-aligned refusal behaviours, and precision-recall tensions emerge between helpfulness and hallucination risk. These findings shift focus from eliminating hallucination to managing it within multi-objective alignment frameworks. Research gaps persist in benchmark coverage, philosophical grounding of the creativity-hallucination boundary, and computational overhead analysis. As LLMs become increasingly deployed in high-stakes domains, achieving principled balance between factual accuracy and creative generation remains central to generative AI research.



## Background

Large language models (LLMs) have achieved remarkable advances in natural language generation. They demonstrate capabilities that range from factual question answering to creative writing. However, these systems exhibit a persistent tendency to produce outputs that are fluent and coherent yet factually inaccurate or unsupported by evidence—a phenomenon widely termed "hallucination" (Alansari & Luqman, 2025). This propensity poses fundamental challenges to the deployment of generative AI in domains requiring high epistemic reliability, such as medical diagnosis, legal reasoning, and knowledge-grounded summarisation. The tension between factual accuracy and generative flexibility have emerged as a central concern in contemporary AI research, with implications for both system design and evaluation methodology.

The origins of hallucination in LLMs can be traced to the probabilistic foundations of language modelling itself. Trained to predict token sequences from vast corpora, these models encode statistical regularities without inherent mechanisms for truth verification or source attribution. Early research distinguished between *intrinsic hallucinations*—outputs inconsistent with provided context—and *extrinsic hallucinations*—claims unsupported by the model's training data (Bang et al., 2025). This taxonomy reflects a broader epistemological distinction. Hallucination concerns consistency with accessible sources, whereas factuality pertains to external truth (Bang et al., 2025). The boundary between intentional creativity and unintentional fabrication remains contested, with Alansari & Luqman (2025) acknowledging a "grey zone" where novelty and accuracy trade off in ways that defy simple categorisation.

Understanding the causal mechanisms underlying hallucination has proven essential for mitigation. Alansari & Luqman (2025) propose a lifecycle framework spanning data collection, architectural design, pretraining, fine-tuning, and inference. They argue that hallucinations emerge from systemic factors rather than isolated decoding failures. Recent work has identified specific mechanisms: Nguyen et al. (2025) attribute faithfulness hallucinations to overconfidence induced by hard-label supervision during fine-tuning, while Zhang et al. (2025) propose a *knowledge overshadowing* hypothesis, wherein dominant knowledge representations suppress less prominent but equally valid information during generation. Notably, Zhang et al. (2025) demonstrate that hallucination rates can increase log-linearly with model size under conditions of knowledge competition, challenging assumptions that scale uniformly improves factual reliability.

The proliferation of hallucination research has been accompanied by methodological advances in detection and evaluation. Traditional overlap metrics such as ROUGE and BLEU prove inadequate for assessing factual consistency. This prompts the development of entailment-based classifiers, uncertainty quantification methods, and self-consistency frameworks (Alansari & Luqman, 2025). Bang et al. (2025) advocate for dynamic test set generation to resist benchmark saturation, while Liu et al. (2025) demonstrate iterative prompt-based refinement using verified question-answer pairs. The field increasingly recognises that robust evaluation requires multiple complementary metrics, with growing reliance on LLM-as-judge frameworks despite acknowledged limitations regarding bias propagation and domain transferability.

Mitigation strategies span the full model lifecycle. Retrieval-augmented generation (RAG) and knowledge graph integration provide external grounding at inference time. Reasoning-based methods such as Chain-of-Thought and self-verification leverage internal consistency checks (Alansari & Luqman, 2025). Training-time interventions include knowledge distillation with soft labels to reduce overconfidence (Nguyen et al., 2025) and decoding-time techniques such as contrastive decoding to amplify overshadowed knowledge (Zhang et al., 2025). However, no single approach robustly prevents hallucination across tasks and domains; hybrid methods combining retrieval, reasoning, and calibration appear most promising (Alansari & Luqman, 2025).

Critically, hallucination mitigation introduces second-order trade-offs that complicate optimisation. Liu et al. (2025) observe that excessive optimisation for faithfulness can reduce informational coverage in summarisation tasks. Meanwhile, Mahmoud et al. (2025) identify a truthfulness-safety trade-off whereby interventions that enhance factual accuracy can degrade safety-aligned refusal behaviours. This latter finding suggests that hallucination and refusal may be entangled in shared representational subspaces. It necessitates multi-objective evaluation frameworks that balance truthfulness, creativity, and alignment. Such interdependencies underscore the inadequacy of treating hallucination as an isolated failure mode and highlight the need for holistic approaches to model evaluation and refinement.

Looking forward, several challenges remain underexplored. Benchmark coverage remains limited for dialogue systems, code generation, and multilingual contexts (Alansari & Luqman, 2025). The boundary between creativity and hallucination lacks philosophical grounding, with creative generation quality measured indirectly through task performance rather than normative criteria. Causally, the mechanisms by which fine-tuning on benign data can degrade orthogonal capabilities—such as refusal or linguistic register maintenance—are poorly understood (Mahmoud et al., 2025). Computational overhead and practical scalability considerations for hybrid mitigation strategies also warrants systematic investigation.

Against this backdrop, the present review addresses the following research question: *How do large language models balance factual accuracy and creative generation, and what methodological approaches have been proposed to mitigate hallucination in Generative AI systems?* This question encompasses both empirical evaluation frameworks and causal mechanisms. It foregrounds the tension between generative flexibility and epistemic reliability that defines contemporary research in this domain. The review synthesises findings from recent benchmark-driven studies, mechanistic investigations, and mitigation proposals, with particular attention to the interdependencies and trade-offs that complicate efforts to align LLM behaviour with human expectations of truthfulness and creativity.

## Review of the Literature

Recent research has disaggregated hallucination into taxonomies, causal mechanisms, and intervention points across the model lifecycle. This review synthesises findings from seven studies. It organises the analysis thematically to highlight convergent insights, methodological tensions, and unresolved challenges.

### Defining and Taxonomising Hallucination

The foundational challenge concerns the boundary between legitimate generative output and epistemic failure. Bang et al. (2025) distinguish *hallucination* (inconsistency with model-accessible sources) from *factuality* (correspondence with external truth). They further subdivide hallucination into *intrinsic* (contradicting provided context) and *extrinsic* (unsupported by training data). Nguyen et al. (2025) alternatively foreground *faithfulness hallucination* (ungrounded from context) versus *factuality hallucination* (world-knowledge contradictions), prioritising the attribution problem. Alansari & Luqman (2025) acknowledge a "grey zone" where intentional creativity and unintentional fabrication become indistinguishable. The lack of consensus reflects deeper questions: should LLMs be evaluated as retrieval systems (where unsupported claims fail) or creative agents (where novelty is valued)?

These definitional disputes carry methodological consequences. Bang et al. (2025) critique benchmarks like TruthfulQA for conflating factuality with hallucination. They argue this misleads model development. Liu et al. (2025) operationalise hallucination narrowly as factual inconsistency via FactCC entailment classifiers. The choice of definition constrains admissible evaluation protocols and progress measurement.

### Causal Mechanisms and Mitigation

Understanding *why* hallucinations occur enables targeted interventions. Alansari & Luqman (2025) propose a lifecycle framework spanning data, architecture, pretraining, fine-tuning, and inference. They argue hallucinations are systemic emergent behaviours. Nguyen et al. (2025) attribute faithfulness failures to *overconfidence* from hard-label supervision during fine-tuning. They demonstrate that knowledge distillation with soft labels reduces hallucination without degrading reasoning capability. Zhang et al. (2025) propose *knowledge overshadowing*—dominant representations suppressing less prominent knowledge—with hallucination rates increasing log-linearly with popularity imbalance, knowledge length, and model size. This scaling law suggests larger models may paradoxically exhibits increased hallucinations under knowledge competition, challenging assumptions that scale uniformly improves reliability.

Mahmoud et al. (2025) identify shared representational entanglement between hallucination and safety-aligned refusal. They demonstrate that overlapping attention heads support both capabilities such that interventions targeting one can degrade the other. This challenges the assumption that hallucination mitigation can proceed independently of other alignment objectives.

Mitigation strategies span the lifecycle. Retrieval-augmented generation (RAG) grounds outputs in external evidence but introduces retrieval quality dependencies (Alansari & Luqman, 2025; Bang et al., 2025). Reasoning-based methods like Chain-of-Thought improve transparency but incur computational costs (Alansari & Luqman, 2025). Training-time interventions modify internal representations: Nguyen et al. (2025) demonstrate knowledge distillation's effectiveness, though teacher access imposes overhead. Decoding-time methods offer training-free alternatives. Zhang et al. (2025) propose CoDA, achieving substantial gains on knowledge-conflict benchmarks. Liu et al. (2025) demonstrate post-hoc iterative refinement via QA pairs, optimal after six iterations but requiring reliable fact-checking oracles. Alansari & Luqman (2025) conclude no single method suffices; hybrid approaches combining techniques show greatest promise.

### Evaluation and Detection

Effective mitigation presupposes reliable detection. Alansari & Luqman (2025) taxonomise approaches: retrieval-based, uncertainty-based, embedding-based, learning-based, and self-consistency-based. Liu et al. (2025) propose answer-first QA generation to reduce evaluation-time hallucinations—a reflexive concern that frameworks themselves may hallucinate. However, reliance on learned classifiers like FactCC introduces error sources and limited generalisability.

Standard overlap metrics (ROUGE, BLEU, BERTScore) miss factuality failures. This drives adoption of NLI-based classifiers and LLM-as-judge frameworks (Alansari & Luqman, 2025). Yet Bang et al. (2025) and Alansari & Luqman (2025) highlight that LLM judges propagate biases, exhibit poor calibration, and fail to transfer across domains. The pragmatic necessity of scalable evaluation has led to widespread GPT-4-based scoring—an assumption warranting empirical validation. Nguyen et al. (2025) employ attention-based detection via Lookback Lens-style classifiers. They offer interpretability but requiring model-specific training. The diversity of paradigms reflects hallucination's multifaceted nature and the absence of gold-standard metrics.

Bang et al. (2025) introduce HalluLens. It combines dynamic test generation (PreciseWikiQA, LongWiki) with static benchmarks to resist saturation and data leakage. Dynamic generation from Wikipedia operationalises extrinsic hallucination, though Wikipedia may not match actual training data. Liu et al. (2025) employ multi-metric evaluation (ROUGE-L, FactCC, attention-based classifiers) across CNN/DailyMail, PubMed, and ArXiv. Alansari & Luqman (2025) highlight uneven coverage. QA and summarisation well-represented, dialogue and code generation underserved, constraining generalisability. Bang et al. (2025) critique factuality benchmarks for conflating factuality with hallucination—a tension between measuring what models *know* versus what they *ground claims in*.

### Trade-offs and Research Gaps

Mitigation introduces second-order trade-offs. Liu et al. (2025) observe excessive faithfulness optimisation reduces informational coverage as models become conservative. Bang et al. (2025) quantify precision-recall tension: answering more increases hallucination risk; refusing more sacrifices helpfulness. No Pareto-optimal solution exists across 13 evaluated LLMs. Mahmoud et al. (2025) identify truthfulness-safety trade-offs. Factual accuracy interventions can degrade refusal behaviours, increasing jailbreak susceptibility. Shared attention heads entangle these capabilities, such that alignment objectives interact rather than optimise independently. Nguyen et al. (2025) offer rare counter-evidence: knowledge distillation reduces hallucination without capability penalties, though generalisability to pretraining and teacher hallucination propagation remain unaddressed.

Zhang et al.'s (2025) scaling law—hallucination rates increase log-linearly with model size under knowledge competition—complicates optimistic scaling narratives. It suggests architectural innovations may be required. However, synthetic experimental conditions raise ecological validity questions for real-world corpora.

Substantial gaps persist. Philosophically, the literature exhibits limited engagement with epistemology or normative frameworks for creative generation (Alansari & Luqman, 2025). The creativity-hallucination boundary is acknowledged but undertheorised. Methodologically, benchmark choices are pragmatic rather than theoretically justified (Liu et al., 2025; Bang et al., 2025). LLM-as-judge reliability across domains is insufficiently validated (Alansari & Luqman, 2025). Human evaluation absence in some studies (Nguyen et al., 2025) limits real-world applicability claims. Task coverage favours summarisation and QA over creative generation, dialogue, and multilingual contexts (Alansari & Luqman, 2025). Causally, knowledge distillation's application to fine-tuning may understate pretraining potential (Nguyen et al., 2025). Quantifying "popularity" in natural text remains challenging (Zhang et al., 2025). Mahmoud et al.'s (2025) entanglement hypothesis requires broader architectural validation. Pragmatically, computational overhead comparisons and multi-objective evaluation frameworks balancing truthfulness, safety, and helpfulness are not standard practice.

### Synthesis

The field transitions from descriptive taxonomy to mechanistic understanding and intervention. Definitional consensus remains elusive. Competing frameworks privilege consistency (Bang et al., 2025), faithfulness (Nguyen et al., 2025), or factuality (Zhang et al., 2025). Causal accounts diverge between overconfidence (Nguyen et al., 2025), knowledge overshadowing (Zhang et al., 2025), and lifecycle factors (Alansari & Luqman, 2025). Yet convergent insights emerge: hallucination is multi-causal, requiring multi-pronged mitigation with hybrid approaches showing promise (Alansari & Luqman, 2025). Evaluation has matured beyond surface metrics toward entailment classifiers, self-consistency checks, and dynamic benchmarking (Bang et al., 2025; Liu et al., 2025). Recognition of trade-offs—accuracy-informativeness (Liu et al., 2025), truthfulness-safety (Mahmoud et al., 2025), refusal-helpfulness (Bang et al., 2025)—shifts framing from eliminating hallucination to *managing* it within multi-objective alignment.

Critically, the field lacks a unified normative stance on generative systems' roles. Are they retrieval systems (where unsupported claims fail), creative agents (where novelty is valued), or hybrid entities navigating context-dependent expectations? Deployment scenarios likely determine the answer, but absent explicit philosophical engagement, progress toward principled evaluation and design is constrained. As research matures, epistemic and normative foundations alongside empirical investigation will be essential for aligning systems with human expectations of truthfulness, creativity, and trustworthiness.


## Conclusion and Discussion

This review has examined how large language models balance factual accuracy and creative generation. It reveals a field transitioning from definitional debates toward mechanistic understanding and practical intervention. Significant progress has been achieved in taxonomic refinement. This distinguishes intrinsic from extrinsic hallucinations, faithfulness from factuality, and creativity from fabrication. Competing causal accounts—overconfidence from hard-label supervision, knowledge overshadowing, and lifecycle-wide systemic factors—provide complementary intervention points. Methodologically, evaluation has matured beyond surface overlap metrics toward entailment classifiers, dynamic benchmarking, and multi-metric frameworks. Mitigation strategies now span the full model lifecycle: retrieval-augmented generation, reasoning-based methods, training interventions, and decoding techniques. The emerging consensus that hybrid approaches show greatest promise reflects pragmatic accommodation to hallucination's multi-causal nature.

Critically, recognition of second-order trade-offs marks a conceptual advance. The field now acknowledge that mitigation introduces precision-recall tensions, accuracy-informativeness trade-offs, and truthfulness-safety entanglements, fundamentally shifting focus from eliminating hallucination to *managing* it within multi-objective alignment frameworks.

Despite progress, substantial limitations persist. Philosophically, the lack of normative frameworks for evaluating creative generation remains problematic. The creativity-hallucination boundary is undertheorised, with evaluation protocols remaining pragmatic rather than principled. Methodologically, benchmark coverage favours QA and summarisation over dialogue, code generation, and multilingual contexts. LLM-as-judge reliability requires systematic validation. Human evaluation absence in some studies limits real-world applicability claims. Causally, proposed mechanisms require broader validation—knowledge distillation's potential at pretraining scale, scaling laws' ecological validity for natural corpora, and representational entanglement across diverse architectures remain underspecified. These limitations arise from the field's relative youth and hallucination's complexity as an umbrella term for distinct failure modes with different aetiologies.

Addressing these gaps requires coordinated advances across multiple dimensions. Philosophically, engagement with epistemology and normative frameworks would ground evaluation principles. Methodologically, expanding benchmark coverage, validating LLM judges systematically, and integrating human evaluation would strengthen validity. Causally, systematic mechanistic investigations across diverse architectures and scales would deepen causal understanding. Pragmatically, comprehensive computational overhead analyses and abstention policies would inform practical deployment decisions.

This review identifies promising MSc research directions. These include developing theoretically grounded evaluation frameworks balancing novelty with accuracy, comparing mitigation strategies' computational trade-offs, investigating representational entanglement using interpretability techniques, extending research to underserved domains, and operationalising multi-objective evaluation frameworks. The fundamental tension between factual accuracy and creative generation will remain central to generative AI research. Progress requires philosophical clarity about system expectations and methodological rigour in measurement. As LLMs become increasingly capable and widely deployed, the stakes of achieving this balance continue to grow.


# References

*All papers can be found in the provided files or on arXiv.*

- Alansari & Luqman (2025) — arXiv:2510.06265v2
- Bang et al. (2025) — arXiv:2504.17550v1
- Liu et al. (2025) — paper found in files
- Nguyen et al. (2025) — arXiv:2502.11306v1
- Zhang et al. (2025) — arXiv:2502.16143v1
- Mahmoud et al. (2025) — arXiv:2510.07775v1